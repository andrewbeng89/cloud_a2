{"name":"Cloud a2","tagline":"SMU IS429 Assignment 2","body":"# Andrew's Big Data Cloud Computing Problem\r\n\r\nThe data I have chosen for this assignment is taken from the [2009 GitHub Contest](https://github.com/blog/466-the-2009-github-contest). The dataset for this competition included the following data files in text format:\r\n\r\n## data.txt ##\r\n\r\nThis is the main dataset.  Each line is of the format <user_id>:<repo_id>\r\nwhich represents a user watching a repository.  There are 440,237 records\r\nin this file, each a single user id and a single repository id seperated\r\nby a colon.  The data looks something like this (information from contest readme):\r\n\r\n\t43642:123344\r\n\t742:22132\r\n\t5414:2373\r\n\t8660:1160\r\n\t10218:409\r\n\t301:6979\r\n\r\n## repos.txt ##\r\n\r\nThis file lists out the 120,867 repositories that are used in the data.txt\r\nset, providing the repository name, date it was created and (if applicable)\r\nthe repository id that it was forked off of.  The data looks like this:\r\n\r\n\t123335:seivan/portfolio_python,2009-02-18\r\n\t123336:sikanrong/Nautilus-OGL,2009-05-19\r\n\t123337:edlebowitz/Downloads,2009-05-05\r\n\t123338:DylanFM/roro-faces,2009-05-31,13635\r\n\t123339:amazingsyco/technicolor-networking,2008-11-22\r\n\t123340:netzpirat/radiant-scoped-admin-extension,2009-02-27,53611\r\n\t123341:panchenliang/tuxedo-bank-server,2009-05-19\r\n\r\n## lang.txt ##\r\n\r\nThe last dataset included is the language breakdown data.  This lists the\r\nlanguages we could identify in each repository - only 73,496 repositories\r\nhave language data that we have calculated, but it is data available to us\r\nso if you want to use it for classifications or something, feel free. Each\r\nline of this file lists the repository id, then a comma delimited list of \r\n\"<lang>;<lines>\" entries containing each major language found and the number\r\nof lines of code for that language in the project.  The data looks like this:\r\n\r\n\t57493:C;29382\r\n\t73920:JavaScript;9759,ActionScript;12781\r\n\t106774:Perl;4449\r\n\t123201:JavaScript;148,Ruby;16079\r\n\t65707:Ruby;29998\r\n\t98561:JavaScript;217,Ruby;4800900\r\n\r\n## The Problem ##\r\n\r\nThe problems I have designed for this assignment are as follows:\r\n\r\n* Which programming language can be found across the most number of repositories?\r\n* How many repositories does this language appear in?\r\n* Based on the data provided, which GitHub repository is the most watched by fellow GitHubbers?\r\n* How many watchers are following this repository?\r\n* What is the primary programming language used in this repository?\r\n\r\n## The BigData Tutorial ##\r\n\r\n### Cloud Platform\r\n\r\nThe public cloud platform that you will be using for this tutorial is [Amazon EC2](http://aws.amazon.com/ec2/) on a single Ubuntu Server instance. Once your EC2 instance has been provisioned and is running, follow these steps:\r\n\r\n1. Install node.js on the EC2 instance by following the steps [here](http://iconof.com/blog/how-to-install-setup-node-js-on-amazon-aws-ec2-complete-guide/)\r\n2. Install mongodb NoSQL database system via the command line:\r\n\t\r\n\t``npm install mongodb -g``\r\n\r\n3. Install git on via the command line:\r\n\r\n\t``sudo apt-get install git``\r\n\r\n4. Setup an account and new database on [MongoLab](https://mongolab.com), and note shell and URI connection parameters\r\n5. From the command line of the EC2 instance, clone this project:\r\n\r\n\t``git clone git://github.com/andrewbeng89/cloud_a2.git``\r\n\t\r\n6. cd to the cloud_a2 project root folder\r\n\r\n---------------------------------------\r\n\r\n### Data Preparation\r\n\r\nI have converted the data.txt and repos.txt files (located in the data directory in this project) to .csv files by finding and replacing all colons with commas and added appropriate first row headers. The conversion to .csv will facilitate the import of the data into the newly created MongoLab database. The data for data.csv now looks like this:\r\n\r\n\tuser_id,repo_id\r\n\t1,1\r\n\t2,2\r\n\t3,3\r\n\t\r\nWhile the data for repos.csv now looks like this:\r\n\r\n\trepo_id,repo_name,date_created,forked_from\r\n\t1,richardc/perl-number-compare,2009-02-26\r\n\t2,axiomsoftware/axiom-inspector,2009-05-09\r\n\t3,rails/open_id_authentication,2008-05-29\r\n\t\r\nThe lang.txt file will converted to lang.json as each repository possible has multiple languages. The conversion to json file will be invoked via a node.js module form the command line:\r\n\r\n\tnode load_lang.js\r\n\r\nThe single row from lang.txt below will correspond to the JSON object beneath it:\r\n\r\n\t8213:Ruby;395056,JavaScript;802\r\n\t\r\n```js\r\n{\r\n    \"repo_id\": \"8213\",\r\n    \"langs\": [\r\n        {\r\n            \"_id\": {\r\n                \"$oid\": \"51243dc0b2f3c8081f000002\"\r\n            },\r\n            \"lang\": \"Ruby\",\r\n            \"lines\": \"395056\"\r\n        },\r\n        {\r\n            \"_id\": {\r\n                \"$oid\": \"51243dc0b2f3c8081f000001\"\r\n            },\r\n            \"lang\": \"JavaScript\",\r\n            \"lines\": \"802\"\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\n---------------------------------------\r\n\r\n### Loading Data into MongoLab\r\n\r\nNow that data.csv, repos.csv and lang.json are prepared, it is time to upload the data to MongoLab Mongo-as-a-Service! I have prepared the mongoinsert_lang_data, mongoinsert_repo_data and mongoinsert_watch_data bash script files to facilitate the batch uploads to MongoLab. However the host, database, user and password parameters will have to be changed to match those from the database you created earlier. For mongoinsert_lang_data, update the parameters accordingly:\r\n\r\n\tmongoimport -h <host_of_created_database> -d <name_of_created_database> -u <user> -p <password> -c langmodels --type json --file ./data/lang.json --jsonArray\r\n\t\r\nOnce the mongodb parameters have been updated for all three bash scripts, upload the data to MongoLab by executing the scripts in the EC2 command line:\r\n\r\n\t./mongoinsert_lang_data\r\n\t./mongoinsert_repo_data\r\n\t./mongoinsert_watch_data\r\n\t\r\nRefreshing the MongoLab admin console for the database will show that three new collections have been made:\r\n\r\n* repomodels: The collection of repository metadata objects corresponding to repos.txt\r\n* langmodels: The collection of repository-languages objects corresponding to lang.txt\r\n* watchmodels: The collection of user_id - repo_id objects of users watching repositories corresponding to data.txt\r\n\r\n---------------------------------------\r\n\r\n### MapReduce with node.js and mongoosejs\r\n\r\nIn order to solve the problems stated earlier, I have created a node.js module that will perform MapReduce on the database collections using the mongoosejs mongodb driver for node. The mongoose database connection parameters in map_reduce.js would have to be updated to those for your database:\r\n\r\n\tmongoose.connect('mongodb://<user>:<password>@<host_of_created_database>/<name_of_created_database>');\r\n\t\r\nOnce this has been updated, via the EC2 command line, execute the module:\r\n\r\n\tnode map_reduce.js\r\n\t\r\nThe result of the operation will be in this format:\r\n\r\n\tFind the programming language that is found \r\n\tacross the most number of repositories:\r\n\tmap reduce took XXXX ms\r\n\tlanguage: XXXXXX, num_repos: XXXXX\r\n\r\n\tFind the repo with the most number of wathcers:\r\n\tmap reduce took XXXXX ms\r\n\tmost number of watchers per repo: XXXX\r\n\trepo_id: XXXXX, repo_name: XXXXXXXXXXXXX\r\n\tprimary programming language of repo: XXXXX\r\n\t\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}